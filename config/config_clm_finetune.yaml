data:
  # train_path: /home/kunato/instruct-transformer/filelist/inst_v1_en_th_train.txt
  # test_path: /home/kunato/instruct-transformer/filelist/inst_v1_en_th_test.txt
  train_path: /home/kunato/instruct-transformer/filelist/v3/deeple_reg.txt
  test_path: /home/kunato/instruct-transformer/filelist/v3/deeple.txt
  config:
    calc_loss_on_pad: False
    override_pad_token: False
  
training_args:
  fp16: True
  num_train_epochs: 1
  logging_steps: 5
  adam_beta1: 0.9
  adam_beta2: 0.95
  max_grad_norm: 0.5
  eval_steps: 200
  save_steps: 1500
  # gradient_accumulation_steps: 1
  # per_device_train_batch_size: 32
  gradient_accumulation_steps: 1 # facebook/xglm-1.7B
  per_device_train_batch_size: 4 # facebook/xglm-1.7B
  warmup_steps: 2
  deepspeed: 'config/ds_config.json' # need to run using deepspeed


# model_name: EleutherAI/pythia-1.4b-deduped
# model_name: sberbank-ai/mGPT
model_name: results/035

lr: 5e-5 # xglm
# lr: 5e-6
