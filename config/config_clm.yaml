data:
  # train_path: /home/kunato/instruct-transformer/filelist/inst_v1_en_th_train.txt
  # test_path: /home/kunato/instruct-transformer/filelist/inst_v1_en_th_test.txt
  train_path: /home/kunato/instruct-transformer/filelist/inst_v1_q_th_a_en_train.txt
  test_path: /home/kunato/instruct-transformer/filelist/inst_v1_q_th_a_en_test.txt
  
training_args:
  fp16: True
  num_train_epochs: 1
  logging_steps: 100
  eval_steps: 200
  save_steps: 1000
  # gradient_accumulation_steps: 1
  # per_device_train_batch_size: 32
  gradient_accumulation_steps: 4 # facebook/xglm-1.7B
  per_device_train_batch_size: 8 # facebook/xglm-1.7B
  warmup_steps: 5
  deepspeed: 'config/ds_config.json' # need to run using deepspeed


# model_name: EleutherAI/pythia-1.4b-deduped
# model_name: sberbank-ai/mGPT
model_name: facebook/xglm-1.7B

lr: 5e-5 # xglm
# lr: 5e-6
