data:
  # train_path: /home/kunato/instruct-transformer/filelist/inst_v1_en_th_train.txt
  # test_path: /home/kunato/instruct-transformer/filelist/inst_v1_en_th_test.txt
  train_path: /home/kunato/instruct-transformer/filelist/v2/inst_v1_th_en_train.txt
  test_path: /home/kunato/instruct-transformer/filelist/v2/inst_v1_th_en_test.txt
  config:
    calc_loss_on_pad: False
    override_pad_token: False
  
training_args:
  fp16: True
  num_train_epochs: 1
  logging_steps: 20
  adam_beta1: 0.9
  adam_beta2: 0.95
  max_grad_norm: 0.5
  eval_steps: 200
  save_steps: 1500
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 32
  # gradient_accumulation_steps: 4 # facebook/xglm-1.7B
  # per_device_train_batch_size: 8 # facebook/xglm-1.7B
  warmup_steps: 20
  deepspeed: 'config/ds_config.json' # need to run using deepspeed


visualize:
  examples:
    - "User: Please recommend me a hotel in bangkok?\n\nDeeple:"
    - "User: แนะนำโรงแรมในกรุงเทพให้หน่อย\n\nDeeple:"
    - "User: Hello\n\nDeeple:"
    - "User: Write me a job application for software engineer\n\nDeeple:"


# model_name: EleutherAI/pythia-1.4b-deduped
# model_name: sberbank-ai/mGPT
model_name: EleutherAI/pythia-70m-deduped

lr: 5e-5 # xglm
# lr: 5e-6
